import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from scipy.signal import savgol_filter
from scipy.ndimage import gaussian_filter1d
import matplotlib.pyplot as plt
import warnings
import random


# ==========================================
# 0. ç¯å¢ƒé…ç½®
# ==========================================
def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True


seed_everything(42)
warnings.filterwarnings("ignore")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

RESULT_DIR = 'Averaged_Interpretability_Final'
os.makedirs(RESULT_DIR, exist_ok=True)
DATA_PATH = 'batch_spectra.csv'


# ==========================================
# 1. æ¨¡å‹æ¶æ„å®šä¹‰
# ==========================================
class ChannelAttention(nn.Module):
    def __init__(self, in_c):
        super().__init__()
        self.fc = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Conv1d(in_c, max(in_c // 4, 1), 1), nn.ReLU(),
            nn.Conv1d(max(in_c // 4, 1), in_c, 1), nn.Sigmoid()
        )

    def forward(self, x): return x * self.fc(x)


class AblationBlock(nn.Module):
    def __init__(self, in_c, out_c, stride=1, use_res=True, use_attn=True):
        super().__init__()
        self.use_res, self.use_attn = use_res, use_attn
        self.conv = nn.Sequential(
            nn.Conv1d(in_c, out_c, 5, stride, 2, bias=False),
            nn.GroupNorm(8, out_c), nn.ReLU(),
            nn.Conv1d(out_c, out_c, 5, 1, 2, bias=False),
            nn.GroupNorm(8, out_c)
        )
        self.attn = ChannelAttention(out_c) if use_attn else nn.Identity()
        self.shortcut = nn.Sequential(
            nn.Conv1d(in_c, out_c, 1, stride, bias=False),
            nn.GroupNorm(8, out_c)
        ) if use_res and (stride != 1 or in_c != out_c) else nn.Identity()

    def forward(self, x):
        out = self.attn(self.conv(x))
        if self.use_res:
            out += self.shortcut(x) if isinstance(self.shortcut, nn.Sequential) else x
        return F.relu(out)


class MBARN_Ablation(nn.Module):
    def __init__(self, use_res, use_attn):
        super().__init__()
        self.stem = nn.Sequential(nn.Conv1d(1, 64, 7, 2, 3), nn.GroupNorm(8, 64), nn.ReLU())
        self.layer = AblationBlock(64, 128, 2, use_res, use_attn)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.heads = nn.ModuleList([nn.Linear(128, 1) for _ in range(3)])

    def forward(self, x):
        x = self.layer(self.stem(x))
        features = x
        x = self.pool(x).flatten(1)
        return [torch.sigmoid(h(x)) for h in self.heads], features


# ==========================================
# 2. Grad-CAM è§£é‡Šå™¨ (åŠ å…¥é«˜æ–¯å¹³æ»‘ä¼˜åŒ–)
# ==========================================
class GradCAM:
    def __init__(self, model):
        self.model = model
        self.gradients = None
        self.features = None

    def save_gradient(self, grad): self.gradients = grad

    def __call__(self, x, label_idx):
        target_layer = self.model.layer.conv[3]
        handler = target_layer.register_backward_hook(lambda m, i, o: self.save_gradient(o[0]))
        out, features = self.model(x)
        self.features = features
        self.model.zero_grad()
        out[label_idx].backward(retain_graph=True)
        weights = torch.mean(self.gradients, dim=2, keepdim=True)
        cam = torch.sum(weights * self.features, dim=1).squeeze().cpu().detach().numpy()
        cam = np.maximum(cam, 0)

        # ç‰©ç†ç‰¹æ€§å¹³æ»‘ï¼šä½¿ç”¨é«˜æ–¯æ»¤æ³¢è®©æ›²çº¿æ›´å¹³æ»‘ï¼Œç¬¦åˆæ‹‰æ›¼å…‰è°±åŒ…ç»œå½¢æ€
        cam = gaussian_filter1d(cam, sigma=2.0)

        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
        handler.remove()
        return np.interp(np.linspace(0, 1, x.shape[2]), np.linspace(0, 1, len(cam)), cam)


# ==========================================
# 3. å¤šæ ·æœ¬å¹³å‡å®éªŒé€»è¾‘
# ==========================================
def run_averaged_interpretability():
    # A. æ•°æ®åŠ è½½
    data_raw = pd.read_csv(DATA_PATH, header=None, low_memory=False)
    W = data_raw.iloc[0, 3:].astype(float).values
    Y_all = data_raw.iloc[1:, 0:3].values.astype(float)
    X_all = data_raw.iloc[1:, 3:].values.astype(float)

    # B. ç­›é€‰ PVC+PMMA æ··åˆæ ·æœ¬é›†åˆ
    target_indices = np.where((Y_all[:, 1] == 1) & (Y_all[:, 2] == 1) & (Y_all[:, 0] == 0))[0]
    if len(target_indices) < 3:  # å¦‚æœçº¯æ··åˆæ ·å¤ªå°‘ï¼Œåˆ™æ”¾å®½æ¡ä»¶
        target_indices = np.where((Y_all[:, 1] == 1) & (Y_all[:, 2] == 1))[0]

    num_to_avg = min(20, len(target_indices))
    selected_indices = target_indices[:num_to_avg]
    print(f"ğŸš€ æ­£åœ¨å¯¹ {num_to_avg} ä¸ªæ··åˆæ ·æœ¬è¿›è¡Œå¹³å‡è§£é‡Šæ€§åˆ†æ...")

    # C. å®éªŒé…ç½®
    configs = [
        {"name": "Baseline (CNN)", "res": False, "attn": False},
        {"name": "CNN + Residual", "res": True, "attn": False},
        {"name": "MBARN (Full)", "res": True, "attn": True}
    ]
    comp_names = ['PS', 'PVC', 'PMMA']
    physical_peaks = {
        'PVC': [637, 695],  # C-Cl stretching
        'PMMA': [812]  # C-H bending, C=O stretching
    }

    # åˆå§‹åŒ–å­˜å‚¨
    accumulated_results = {cfg['name']: {1: [], 2: []} for cfg in configs}
    accumulated_spectra = []

    # D. å¾ªç¯è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„è§£é‡Šçƒ­åŠ›å›¾
    for idx in selected_indices:
        # å•æ ·æœ¬é¢„å¤„ç†
        x_raw = X_all[idx]
        x_proc = x_raw - savgol_filter(x_raw, 51, 3)
        x_proc = (x_proc - x_proc.min()) / (x_proc.max() - x_proc.min() + 1e-8)
        accumulated_spectra.append(x_proc)

        input_tensor = torch.from_numpy(x_proc).float().unsqueeze(0).unsqueeze(0).to(device)

        for cfg in configs:
            model = MBARN_Ablation(cfg['res'], cfg['attn']).to(device)
            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
            # é’ˆå¯¹æ€§æ‹Ÿåˆè¯¥æ ·æœ¬
            for _ in range(50):
                model.train()
                out, _ = model(input_tensor)
                loss = sum(F.binary_cross_entropy(out[i].view(-1),
                                                  torch.tensor([Y_all[idx, i]], device=device).float()) for i in
                           range(3))
                loss.backward();
                optimizer.step();
                optimizer.zero_grad()

            model.eval()
            gcam = GradCAM(model)
            accumulated_results[cfg['name']][1].append(gcam(input_tensor, label_idx=1))  # PVC
            accumulated_results[cfg['name']][2].append(gcam(input_tensor, label_idx=2))  # PMMA

    # E. ç»“æœç»˜å›¾
    avg_spectrum = np.mean(accumulated_spectra, axis=0)

    # å¢åŠ  figsize ç¡®ä¿å¤§å­—å·ä¸‹å¸ƒå±€ä¸æ‹¥æŒ¤
    fig, axes = plt.subplots(2, 1, figsize=(14, 12), sharex=True)
    colors = {'Baseline (CNN)': '#4C72B0', 'CNN + Residual': '#E1812C', 'MBARN (Full)': '#55A868'}

    for b_idx, ax in enumerate(axes):
        t_idx = b_idx + 1  # å¯¹åº” PVC=1, PMMA=2
        t_name = comp_names[t_idx]

        # 1. ç»˜åˆ¶å¹³å‡å…‰è°±èƒŒæ™¯ (ç¨å¾®åŠ ç²—)
        ax.plot(W, avg_spectrum, color='black', alpha=0.15, label='Mean Mixed Spectrum', linewidth=1.5)

        # 2. æ ‡æ³¨ç‰©ç†å‚è€ƒçº¿
        for peak in physical_peaks[t_name]:
            ax.axvline(x=peak, color='red', linestyle='--', alpha=0.5, linewidth=2,
                       label='Characteristic Peak' if peak == physical_peaks[t_name][0] else "")

        # 3. ç»˜åˆ¶å„æ¨¡å‹å¹³å‡ Attention
        for name in accumulated_results:
            mean_cam = np.mean(accumulated_results[name][t_idx], axis=0)
            # åŠ ç²—æ›²çº¿ä»¥é…åˆå¤§å­—å·
            ax.plot(W, mean_cam, label=f'{name} Focus', color=colors[name], linewidth=3.0)
            if "Full" in name:
                ax.fill_between(W, 0, mean_cam, color=colors[name], alpha=0.15)

        # --- å­—ä½“ä¸åˆ»åº¦è°ƒä¼˜ ---
        # è®¾ç½®å­å›¾æ ‡é¢˜ (åŠ ç²—, å­—å·18)
        ax.set_title(f"Targeting {t_name} Fingerprints (Averaged over {num_to_avg} samples)",
                     fontsize=20, fontweight='bold', pad=15)

        # è®¾ç½®çºµè½´æ ‡ç­¾ (å­—å·16)
        ax.set_ylabel("Attention Score", fontsize=18, labelpad=10)

        # è®¾ç½®åˆ»åº¦æ•°å­—å¤§å° (å­—å·15)
        ax.tick_params(axis='both', which='major', labelsize=15)

        # è®¾ç½®å›¾ä¾‹ (å­—å·14, è®¾ç½®èƒŒæ™¯æ¡†æé«˜å¯è¯»æ€§)
        ax.legend(loc='upper right', fontsize=13, frameon=True, shadow=True, facecolor='white')

        # ç½‘æ ¼çº¿ç¨å¾®æ˜æ˜¾ä¸€ç‚¹
        ax.grid(True, alpha=0.4, linestyle=':')

    # è®¾ç½®æ¨ªè½´æ ‡ç­¾ (å­—å·18)
    plt.xlabel("Raman Shift ($cm^{-1}$)", fontsize=18, labelpad=10)

    # è°ƒæ•´æ•´ä½“å¸ƒå±€ï¼Œé˜²æ­¢æ ‡ç­¾è£å‰ª
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

    # ä¿å­˜é«˜åˆ†è¾¨ç‡å›¾ç‰‡
    save_path = os.path.join(RESULT_DIR, 'PVC_PMMA_Averaged_Interpretation_LargeFont.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å›¾åƒå·²ä¿å­˜è‡³: {save_path}")
    plt.show()


if __name__ == "__main__":
    run_averaged_interpretability()